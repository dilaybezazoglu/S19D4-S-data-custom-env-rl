{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§± Ã–zel Ortam\n",
    "\n",
    "---\n",
    "\n",
    "Bu gÃ¶revde, derste Ã¶rnek olarak kullanÄ±lan **Ä±zgara tabanlÄ± navigasyon ortamÄ±nÄ±n** birebir aynÄ±sÄ±nÄ± tasarlayÄ±p uygulayacaksÄ±nÄ±z.\n",
    "\n",
    "- Ajan Ä±zgaranÄ±n bir kÃ¶ÅŸesinde baÅŸlar.\n",
    "- AmaÃ§ karÅŸÄ± kÃ¶ÅŸeye ulaÅŸmaktÄ±r.\n",
    "- Izgarada ajanÄ±n kaÃ§Ä±nmasÄ± gereken engeller veya \"delikler\" bulunabilir.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ AmaÃ§lar\n",
    "\n",
    "- ğŸ“ **OrtamÄ± TanÄ±mlayÄ±n**: ajanÄ±n baÅŸlangÄ±Ã§tan hedefe doÄŸru hareket ettiÄŸi bir Ä±zgara dÃ¼nyasÄ± oluÅŸturun.\n",
    "- âš™ï¸ **Ortam Dinamiklerini UygulayÄ±n**: hareket ve Ã¶dÃ¼l atamasÄ± kurallarÄ±nÄ± programlayÄ±n (Ã¶rn. deliklere dÃ¼ÅŸmek iÃ§in ceza, hedefe ulaÅŸmak iÃ§in Ã¶dÃ¼l).\n",
    "- ğŸ‘ï¸ **GÃ¶zlem ve Eylemleri AyarlayÄ±n**:\n",
    "    - **GÃ¶zlem alanÄ±** â†’ AjanÄ±n algÄ±ladÄ±ÄŸÄ± ÅŸeyler (Ã¶rn. Ä±zgaradaki pozisyonu)  \n",
    "    - **Eylem alanÄ±** â†’ AjanÄ±n yapabileceÄŸi ÅŸeyler (Ã¶rn. yukarÄ±, aÅŸaÄŸÄ±, sola, saÄŸa hareket)\n",
    "- ğŸ–¼ï¸ **Render Metodu Ekleyin**: ortamÄ± gÃ¶rselleÅŸtirmek iÃ§in `.render()` fonksiyonu ekleyin â€” hata ayÄ±klama ve ajanÄ±n davranÄ±ÅŸÄ±nÄ± anlama iÃ§in faydalÄ±dÄ±r.\n",
    "- ğŸ§© **Ã–zel Ã–zellikler Ekleyin**: derste ele alÄ±nan engeller, Ã§ukurlar veya diÄŸer Ã¶zellikleri dahil ederek ortamÄ±nÄ±zÄ± daha dinamik ve gerÃ§ekÃ§i hale getirin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Bu gÃ¶rev iÃ§in ihtiyacÄ±mÄ±z olan tÃ¼m paketleri iÃ§e aktararak baÅŸlayalÄ±m:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Dict, Tuple\n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ§© BÃ¶lÃ¼m 1: Ã–zel Ortam SÄ±nÄ±fÄ± OluÅŸturma\n",
    "\n",
    "Bu bÃ¶lÃ¼mde, kÄ±smen yazÄ±lmÄ±ÅŸ bir sÄ±nÄ±fÄ± tamamlayarak Ã¶zel ortamÄ±nÄ±zÄ± uygulayacaksÄ±nÄ±z.  \n",
    "YapÄ±, Gymnasium'un ortam oluÅŸturma iÃ§in standart formatÄ±nÄ± takip eder.\n",
    "\n",
    "### ğŸ› ï¸ YapacaklarÄ±nÄ±z\n",
    "\n",
    "- BazÄ± temel metotlarÄ± Ã¶nceden yazÄ±lmÄ±ÅŸ bir ÅŸablon saÄŸlanmÄ±ÅŸtÄ±r.  \n",
    "- `# CODE HERE` yazdÄ±ÄŸÄ±nÄ± gÃ¶rdÃ¼ÄŸÃ¼nÃ¼z her yerde, eksik mantÄ±ÄŸÄ± doldurmanÄ±z gerekir.  \n",
    "- Her adÄ±mda size rehberlik etmesi iÃ§in satÄ±r iÃ§i ipuÃ§larÄ± verilmiÅŸtir.\n",
    "\n",
    "### ğŸ¯ Hedefiniz\n",
    "\n",
    "- SÄ±nÄ±fÄ± gerÃ§ek bir Gymnasium ortamÄ± gibi davranacak ÅŸekilde tamamlayÄ±n.  \n",
    "- ÅunlarÄ± yapmalÄ±:\n",
    "  - GeÃ§erli bir gÃ¶zlem ve eylem alanÄ± tanÄ±mlamalÄ±  \n",
    "  - Ajan hareketini ve geÃ§iÅŸleri ele almalÄ±  \n",
    "  - Ã–dÃ¼lleri dÃ¶ndÃ¼rmeli ve `done` bayraklarÄ±nÄ± uygun ÅŸekilde gÃ¼ncellemeli  \n",
    "  - Ã‡alÄ±ÅŸan bir `.reset()` ve `.step()` metodu iÃ§ermeli  \n",
    "  - Ä°steÄŸe baÄŸlÄ± olarak gÃ¶rselleÅŸtirme iÃ§in basit bir `.render()` iÃ§ermeli\n",
    "\n",
    "ğŸ§  Her metodu anlamak iÃ§in zaman ayÄ±rÄ±n â€” Ã¶zellikle durum geÃ§iÅŸlerinin ve Ã¶dÃ¼llerin nasÄ±l yÃ¶netildiÄŸini. RL ortamlarÄ± burada canlanÄ±r.\n",
    "\n",
    "ğŸ“š Ortam yapÄ±sÄ± ve en iyi uygulamalar hakkÄ±nda ayrÄ±ntÄ±lÄ± adÄ±mlar iÃ§in resmi [Gymnasium Ã¶zel ortam kÄ±lavuzuna](https://gymnasium.farama.org/introduction/create_custom_env/) baÅŸvurun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGridEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "        # self.size = # Size of the square grid\n",
    "        # self.action_space = # Define the action space to have four possible actions: right, up, left, down\n",
    "        # self.agent_position = # Initial position of the agent (top-left corner)\n",
    "        # self.goal_position =  # Position of the target (bottom-right corner)\n",
    "        # self.hole_position =  # Position of the hole (bottom middle)\n",
    "        # self.action_to_direction = # Mapping from actions to grid movements\n",
    "        pass  # YOUR CODE HERE\n",
    "\n",
    "        # Define the observation space using dictionary spaces for agent and target locations\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"agent\": spaces.Box(low=0, high=self.size - 1, shape=(2,), dtype=np.int32),\n",
    "            \"target\": spaces.Box(low=0, high=self.size - 1, shape=(2,), dtype=np.int32),\n",
    "        })\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[Dict, Dict]:\n",
    "        # Reset the environment for a new episode\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # self.agent_position = # Reset the agent's position to the starting position\n",
    "        pass  # YOUR CODE HERE\n",
    "\n",
    "        # Goal position remains constant\n",
    "        self.goal_position = np.array([2, 2], dtype=np.int32)\n",
    "\n",
    "        # Generate observation and info for the current state\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        return observation, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # Return current observation of agent and target positions\n",
    "        return {\n",
    "            \"agent\": self.agent_position.copy(),\n",
    "            \"target\": self.goal_position.copy()\n",
    "        }\n",
    "\n",
    "    def _get_info(self):\n",
    "        # Compute the Manhattan distance between the agent and the target\n",
    "        distance = np.sum(np.abs(self.agent_position - self.goal_position))\n",
    "        return {\"distance\": distance}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Apply the action to change the agent_position\n",
    "        # If agent fell in hole, give -10 reward, and set done=True\n",
    "        # elif agent reached goal, give +10 reward, and set done=True\n",
    "        # else, give -1 reward, and set done=False\n",
    "        pass  # YOUR CODE HERE\n",
    "\n",
    "        # Collect observation and info about the current state\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        return observation, reward, done, False, info  # 'truncated' is always False\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "\n",
    "        # Visual representation of the grid\n",
    "        grid = np.full((self.size, self.size), fill_value=' ')\n",
    "\n",
    "        # Get x and y indexes for agent_position, goal_position, and hole_position\n",
    "        pass  # YOUR CODE HERE\n",
    "\n",
    "        grid[agent_x][agent_y] = 'A'\n",
    "        grid[goal_x][goal_y] = 'G'\n",
    "        grid[hole_x][hole_y] = 'H'\n",
    "\n",
    "        # Print the grid with borders\n",
    "        print(\"+---\" * self.size + \"+\")\n",
    "        for row in grid:\n",
    "            print(\"|\" + \"|\".join(f\" {cell} \" for cell in row) + \"|\")\n",
    "            print(\"+---\" * self.size + \"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘‡ Ã–nceki bÃ¶lÃ¼mÃ¼ doÄŸru tamamladÄ±ÄŸÄ±nÄ±zÄ± test etmek iÃ§in aÅŸaÄŸÄ±daki hÃ¼creyi Ã§alÄ±ÅŸtÄ±rÄ±n. DoÄŸru yaptÄ±ysanÄ±z, ajanÄ±n bir bÃ¶lÃ¼mÃ¼n sonu olan hedefe veya deliÄŸe ulaÅŸana kadar ortamÄ±nÄ±zda rastgele hareket ettiÄŸini gÃ¶receksiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the custom environment\n",
    "env = CustomGridEnv()\n",
    "\n",
    "# Reset the environment to its initial state and get the initial observation and info\n",
    "obs, info = env.reset()\n",
    "print(\"Initial Observation:\", obs)  # Display the initial position of the agent and the target\n",
    "print(\"Initial Info:\", info)  # Display additional information such as the distance from the target\n",
    "\n",
    "# Loop through a maximum of 100 steps\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()  # Randomly sample an action from the action space\n",
    "    obs, reward, done, info, _ = env.step(action)  # Apply the action and get the results\n",
    "    env.render()  # Render the current state of the environment to visualize the agent's position\n",
    "\n",
    "    # Print the current state, reward received, whether the episode is done, and any additional info\n",
    "    print(f\"State: {obs}, Reward: {reward}, Done: {done}, Info: {info}\")\n",
    "\n",
    "    # If the episode is finished (agent reached the goal or fell into a hole), exit the loop\n",
    "    if done:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BÃ¶lÃ¼m 2: DQN EÄŸitimi ğŸ¤–\n",
    "\n",
    "Bu bÃ¶lÃ¼mde, Ã¶zel ortamÄ±nÄ±zÄ± baÅŸlatacak ve Stable Baselines3 kÃ¼tÃ¼phanesini kullanarak bir DQN ajanÄ± ile etkileÅŸim iÃ§in hazÄ±rlayacaksÄ±nÄ±z. Temel gÃ¶rev, ortamÄ±nÄ±zÄ±n kÃ¼tÃ¼phanenin gereksinimleriyle uyumlu olduÄŸundan emin olmaktÄ±r, bu da onu uygun ÅŸekilde sarmalamayÄ± iÃ§erir.\n",
    "\n",
    "#### ğŸ“ Ä°zlenecek adÄ±mlar\n",
    "\n",
    "1. ğŸ§± **Ã–zel OrtamÄ± BaÅŸlatÄ±n**: Ã¶zel ortam sÄ±nÄ±fÄ±nÄ±zÄ±n bir Ã¶rneÄŸini oluÅŸturun.\n",
    "2. ğŸ” **SB3 UyumluluÄŸunu SaÄŸlayÄ±n**: Stable Baselines3'ten `make_vec_env` fonksiyonunu kullanarak ortamÄ±nÄ±zÄ± sarÄ±n, bÃ¶ylece kÃ¼tÃ¼phanenin vektÃ¶rleÅŸtirilmiÅŸ ortam gereksinimleriyle uyumlu hale getirin.\n",
    "3. âš™ï¸ **DQN AjanÄ±nÄ± YapÄ±landÄ±rÄ±n ve EÄŸitin**: DQN ajanÄ±nÄ± uygun hiperparametrelerle kurun ve ortamÄ±nÄ±zda eÄŸitin.\n",
    "4. ğŸ“Š **EÄŸitim Ä°lerlemesini Ä°zleyin**: AjanÄ±n Ã¶ÄŸrenme ilerlemesini ve performansÄ±nÄ± zaman iÃ§inde gÃ¶zlemlemek iÃ§in gÃ¼nlÃ¼kleme ve izleme uygulayÄ±n.\n",
    "5. ğŸ’¾ **Modeli kaydedin**: EÄŸitim sonrasÄ±nda modelinizi kaydedin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ® BÃ¶lÃ¼m 3: EÄŸitilmiÅŸ modelinizi kullanÄ±n\n",
    "\n",
    "EÄŸitilmiÅŸ DQN modelinizi yÃ¼kleyecek ve ortamda karar vermek iÃ§in kullanacaksÄ±nÄ±z, baÅŸlangÄ±Ã§ noktasÄ±ndan hedefe delikleri kaÃ§Ä±narak nasÄ±l gittiÄŸini gÃ¶zlemleyeceksiniz. Bu sadece eÄŸitimin etkinliÄŸini doÄŸrulamanÄ±za izin vermeyecek, aynÄ± zamanda ajanÄ±n karar verme sÃ¼recini gÃ¶rsel olarak yorumlamanÄ±za da olanak saÄŸlayacaktÄ±r.\n",
    "\n",
    "### Ä°zlenecek adÄ±mlar ğŸ“\n",
    "1. ğŸ’¾ **EÄŸitilmiÅŸ Modeli YÃ¼kleyin**: EÄŸitim aÅŸamasÄ±nda kaydedilen DQN modelini alÄ±n.\n",
    "2. ğŸ”„ **OrtamÄ± SÄ±fÄ±rlayÄ±n**: Navigasyon gÃ¶revini sÄ±fÄ±rdan baÅŸlatmak iÃ§in ortamÄ± baÅŸlatÄ±n.\n",
    "3. ğŸ§  **Navigasyon SimÃ¼lasyonunu Ã‡alÄ±ÅŸtÄ±rÄ±n**: Modeli kullanarak ortamÄ±n durumlarÄ±na dayalÄ± eylemleri tahmin edin ve ajanÄ±n attÄ±ÄŸÄ± her adÄ±mÄ± gÃ¶rselleÅŸtirin.\n",
    "4. ğŸ‘€ **Her AdÄ±mÄ± GÃ¶rselleÅŸtirin**: AjanÄ±n pozisyonunu, hedefi ve herhangi bir engeli veya deliÄŸi gÃ¶steren Ä±zgaranÄ±n basit bir gÃ¶rselleÅŸtirmesini uygulayÄ±n.\n",
    "5. ğŸ“ **Ajan DavranÄ±ÅŸÄ±nÄ± Analiz Edin**: AjanÄ±n hedefe ulaÅŸma yeteneÄŸini gÃ¶zlemleyin ve not edin ve delikleri ne kadar etkili bir ÅŸekilde kaÃ§Ä±ndÄ±ÄŸÄ±nÄ± gÃ¶rÃ¼n.\n",
    "\n",
    "Kodun bir kÄ±smÄ± zaten orada ve `# CODE HERE` yorumunu gÃ¶rdÃ¼ÄŸÃ¼nÃ¼z her yerde kod doldurmanÄ±z gerekiyor. BaÅŸlamanÄ±z iÃ§in bazÄ± ipuÃ§larÄ± bulacaksÄ±nÄ±z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "pass  # YOUR CODE HERE\n",
    "\n",
    "\n",
    "obs = env.reset()  # Reset the environment to the initial state and get the first observation\n",
    "\n",
    "for _ in range(200):  # Loop through a maximum of 200 steps (or until the episode ends)\n",
    "    # Use the model to predict the next action to be taken, based on the current observation\n",
    "    # Execute the chosen action in the environment, receive the next state and reward, and check if the episode is done\n",
    "    pass  # YOUR CODE HERE\n",
    "\n",
    "    # Initialize a 3x3 grid filled with spaces to represent the environment visually\n",
    "    grid = np.full((3,3 ), fill_value=' ')\n",
    "\n",
    "    # Extract coordinates for the agent, goal, and hole using the current observation\n",
    "    agent_x, agent_y = obs['agent'][0][0],obs['agent'][0][1]\n",
    "    goal_x, goal_y = obs['target'][0][0],obs['target'][0][1]\n",
    "    hole_x, hole_y = 2,1  # The hole's position is fixed in the environment\n",
    "\n",
    "    # Update the grid with the agent's, goal's, and hole's positions\n",
    "    grid[agent_x][agent_y] = 'A'  # Mark the agent's position with 'A'\n",
    "    grid[goal_x][goal_y] = 'G'  # Mark the goal position with 'G'\n",
    "    grid[hole_x][hole_y] = 'H'  # Mark the hole position with 'H'\n",
    "\n",
    "    # Print the current state, reward received, if the episode is done, and any additional info\n",
    "    print(f\"State: {obs}, Reward: {rewards}, Done: {done}, Info: {info}\")\n",
    "\n",
    "    # Print the grid visually in the console\n",
    "    print(\"+---\" * 3 + \"+\")  # Top border of the grid\n",
    "    for row in grid:\n",
    "        print(\"|\" + \"|\".join(f\" {cell} \" for cell in row) + \"|\")  # Print each row with cells separated by '|'\n",
    "        print(\"+---\" * 3 + \"+\")  # Separator border after each row\n",
    "\n",
    "    # If the episode is finished (either the goal is reached or the agent fell into the hole), reset the environment\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ§  ArtÄ±k ajanÄ±nÄ±zÄ±n ortamda hareket ettiÄŸini gÃ¶rmelisiniz.\n",
    "\n",
    "EÄŸer ajan **belirli eylemleri tekrarlayarak takÄ±lÄ±p kalÄ±rsa** veya hedefe ulaÅŸamazsa, muhtemelen **yeterince uzun eÄŸitilmediÄŸi** iÃ§indir.\n",
    "\n",
    "EÄŸitim adÄ±m sayÄ±sÄ±nÄ± artÄ±rmayÄ± deneyin ve modeli yeniden eÄŸitin â€” daha uzun eÄŸitim genellikle ajanÄ±n daha iyi bir politika Ã¶ÄŸrenmesine yardÄ±mcÄ± olur."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
